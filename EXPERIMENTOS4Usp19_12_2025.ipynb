{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experimento 4"
      ],
      "metadata": {
        "id": "UCQv0llg6YW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install minisom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEz7Mki0QTO-",
        "outputId": "6b3a07e4-cfbe-4d17-f3ce-0aba741fe16a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minisom\n",
            "  Downloading minisom-2.3.5.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: minisom\n",
            "  Building wheel for minisom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minisom: filename=MiniSom-2.3.5-py3-none-any.whl size=12031 sha256=5cdece4ef1c1a2d4df3090a9c4ceccae2b58c776a8b32b76d9806ba796974422\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/8c/a4/5b7aa56fa6ef11d536d45da775bcc5a2a1c163ff0f8f11990b\n",
            "Successfully built minisom\n",
            "Installing collected packages: minisom\n",
            "Successfully installed minisom-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# IMPORTS\n",
        "# ==========================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from minisom import MiniSom\n",
        "\n",
        "# ==========================================================\n",
        "# CONFIGURAÇÕES GERAIS\n",
        "# ==========================================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 128\n",
        "epochs_cnn = 10\n",
        "\n",
        "num_classes = 10\n",
        "min_cluster_size = 50\n",
        "\n",
        "# Variação do SOM (atividade do aluno)\n",
        "som_sizes = [(5, 5), (10, 10), (15, 15)]\n",
        "\n",
        "# ==========================================================\n",
        "# CNN PARA CLASSIFICAÇÃO E EXTRAÇÃO DE FEATURES\n",
        "# ==========================================================\n",
        "class CNNFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.fc = nn.Linear(64 * 7 * 7, num_classes)\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        features = x.view(x.size(0), -1)\n",
        "\n",
        "        if return_features:\n",
        "            return features\n",
        "\n",
        "        return self.fc(features)\n",
        "\n",
        "# ==========================================================\n",
        "# TREINAMENTO DA CNN\n",
        "# ==========================================================\n",
        "def train_cnn(model, loader):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs_cnn):\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"[CNN] Epoch {epoch+1}/{epochs_cnn} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "# ==========================================================\n",
        "# EXTRAÇÃO DE FEATURES\n",
        "# ==========================================================\n",
        "def extract_features(model, loader):\n",
        "    model.eval()\n",
        "    X, y = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            feats = model(images, return_features=True)\n",
        "\n",
        "            X.append(feats.cpu().numpy())\n",
        "            y.append(labels.numpy())\n",
        "\n",
        "    return np.vstack(X), np.hstack(y)\n",
        "\n",
        "# ==========================================================\n",
        "# MATRIZ DE CONFUSÃO\n",
        "# ==========================================================\n",
        "def plot_confusion(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=range(num_classes))\n",
        "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "    plt.title(title)\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================================\n",
        "# PUREZA DOS CLUSTERS\n",
        "# ==========================================================\n",
        "def cluster_purity(clusters, labels):\n",
        "    purity = {}\n",
        "    unique_clusters = np.unique(clusters, axis=0)\n",
        "\n",
        "    for c in unique_clusters:\n",
        "        idx = np.all(clusters == c, axis=1)\n",
        "        labels_c = labels[idx]\n",
        "\n",
        "        if len(labels_c) == 0:\n",
        "            continue\n",
        "\n",
        "        counts = np.bincount(labels_c, minlength=num_classes)\n",
        "        purity[(c[0], c[1])] = counts.max() / counts.sum()\n",
        "\n",
        "    return purity\n",
        "\n",
        "# ==========================================================\n",
        "# ASSOCIAÇÃO AMOSTRA → CLUSTER SOM\n",
        "# ==========================================================\n",
        "def som_clusters(som, X):\n",
        "    return np.array([som.winner(x) for x in X])\n",
        "\n",
        "# ==========================================================\n",
        "# PREDIÇÃO COM MODELOS LOCAIS (SOM)\n",
        "# ==========================================================\n",
        "def predict_som_models(X, clusters, models, fallback):\n",
        "    preds = []\n",
        "    for x, c in zip(X, clusters):\n",
        "        model = models.get(tuple(c))\n",
        "        if model:\n",
        "            preds.append(model.predict([x])[0])\n",
        "        else:\n",
        "            preds.append(fallback.predict([x])[0])\n",
        "    return np.array(preds)\n",
        "\n",
        "# ==========================================================\n",
        "# DATASET MNIST\n",
        "# ==========================================================\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = datasets.MNIST(\"data\", train=True, download=True, transform=transform)\n",
        "test_data  = datasets.MNIST(\"data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ==========================================================\n",
        "# 1) TREINAMENTO DA CNN\n",
        "# ==========================================================\n",
        "cnn = CNNFeatureExtractor().to(device)\n",
        "train_cnn(cnn, train_loader)\n",
        "\n",
        "# ==========================================================\n",
        "# BASELINE CNN\n",
        "# ==========================================================\n",
        "cnn.eval()\n",
        "y_pred_cnn = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, _ in test_loader:\n",
        "        x = x.to(device)\n",
        "        logits = cnn(x)\n",
        "        y_pred_cnn.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "\n",
        "y_test_img = test_data.targets.numpy()\n",
        "plot_confusion(y_test_img, y_pred_cnn, \"CNN – Dados Reais\")\n",
        "\n",
        "# ==========================================================\n",
        "# 2) EXTRAÇÃO DE FEATURES\n",
        "# ==========================================================\n",
        "X_train, y_train = extract_features(cnn, train_loader)\n",
        "X_test,  y_test  = extract_features(cnn, test_loader)\n",
        "\n",
        "# ==========================================================\n",
        "# 3) ENSEMBLES GLOBAIS\n",
        "# ==========================================================\n",
        "bagging_mlp = BaggingClassifier(\n",
        "    estimator=MLPClassifier(hidden_layer_sizes=(128,), max_iter=300),\n",
        "    n_estimators=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_mlp.fit(X_train, y_train)\n",
        "\n",
        "boosting_mlp = AdaBoostClassifier(\n",
        "    estimator=MLPClassifier(hidden_layer_sizes=(128,), max_iter=200),\n",
        "    n_estimators=2\n",
        ")\n",
        "boosting_mlp.fit(X_train, y_train)\n",
        "\n",
        "svm_global = SVC(kernel=\"rbf\", gamma=\"scale\")\n",
        "svm_global.fit(X_train, y_train)\n",
        "\n",
        "# ==========================================================\n",
        "# 4) SOM + ESPECIALISTAS LOCAIS\n",
        "# ==========================================================\n",
        "results = []\n",
        "\n",
        "for grid in som_sizes:\n",
        "    print(f\"\\n===== SOM {grid[0]}x{grid[1]} =====\")\n",
        "\n",
        "    som = MiniSom(\n",
        "        grid[0], grid[1],\n",
        "        X_train.shape[1],\n",
        "        sigma=1.0,\n",
        "        learning_rate=0.5\n",
        "    )\n",
        "\n",
        "    som.random_weights_init(X_train)\n",
        "    som.train_random(X_train, 5000)\n",
        "\n",
        "    train_clusters = som_clusters(som, X_train)\n",
        "    test_clusters  = som_clusters(som, X_test)\n",
        "\n",
        "    # ----------------------\n",
        "    # Pureza dos clusters\n",
        "    # ----------------------\n",
        "    purity = cluster_purity(train_clusters, y_train)\n",
        "    avg_purity = np.mean(list(purity.values()))\n",
        "    print(f\"Pureza média dos clusters: {avg_purity:.3f}\")\n",
        "\n",
        "    # ----------------------\n",
        "    # SVM por cluster\n",
        "    # ----------------------\n",
        "    cluster_svms = {}\n",
        "    unique_clusters = np.unique(train_clusters, axis=0)\n",
        "\n",
        "    for cluster in unique_clusters:\n",
        "        idx = np.all(train_clusters == cluster, axis=1)\n",
        "        if np.sum(idx) < min_cluster_size:\n",
        "            continue\n",
        "\n",
        "        svm = SVC(kernel=\"rbf\", gamma=\"scale\")\n",
        "        svm.fit(X_train[idx], y_train[idx])\n",
        "        cluster_svms[tuple(cluster)] = svm\n",
        "\n",
        "    y_pred = predict_som_models(\n",
        "        X_test,\n",
        "        test_clusters,\n",
        "        cluster_svms,\n",
        "        svm_global\n",
        "    )\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Acurácia SOM + SVM: {acc:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "        \"som\": f\"{grid[0]}x{grid[1]}\",\n",
        "        \"purity\": avg_purity,\n",
        "        \"accuracy\": acc\n",
        "    })\n",
        "\n",
        "    plot_confusion(y_test, y_pred, f\"SOM {grid[0]}x{grid[1]} + SVM\")\n",
        "\n",
        "# ==========================================================\n",
        "# RESUMO FINAL\n",
        "# ==========================================================\n",
        "print(\"\\n===== RESULTADOS FINAIS =====\")\n",
        "for r in results:\n",
        "    print(\n",
        "        f\"SOM {r['som']} | \"\n",
        "        f\"Pureza média: {r['purity']:.3f} | \"\n",
        "        f\"Acurácia: {r['accuracy']:.4f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3u4exf1Pec-",
        "outputId": "df466bae-fa2b-4410-e1f6-a7d246522fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 51.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 2.26MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.0MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.2MB/s]\n"
          ]
        }
      ]
    }
  ]
}